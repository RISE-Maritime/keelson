#!/usr/bin/env python3

import json
import time
import atexit
import logging
import pathlib
import warnings
import argparse
from queue import Queue, Empty
from threading import Thread, Event
from typing import Dict
from contextlib import contextmanager


import zenoh
from mcap.writer import Writer
from mcap.well_known import SchemaEncoding, MessageEncoding
from google.protobuf.message import DecodeError

from collections import Counter

import keelson

logger = logging.getLogger("mcap-record")

MAIN_LOOP_SLEEP_TIME = 10.0  # seconds


def main():
    parser = argparse.ArgumentParser(
        prog="mcap-record",
        description="A pure python mcap recorder for keelson",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    parser.add_argument("--log-level", type=int, default=logging.INFO)

    parser.add_argument(
        "--mode",
        "-m",
        dest="mode",
        choices=["peer", "client"],
        type=str,
        help="The zenoh session mode.",
    )

    parser.add_argument(
        "--connect",
        action="append",
        type=str,
        help="Endpoints to connect to, in case multicast is not working. ex. tcp/localhost:7447",
    )

    parser.add_argument(
        "-k",
        "--key",
        type=str,
        action="append",
        required=True,
        help="Key expressions to subscribe to from the Zenoh session",
    )

    parser.add_argument(
        "-o",
        "--output",
        type=pathlib.Path,
        required=True,
        help="File path to write recording to",
    )

    parser.add_argument(
        "--suffix-with-datetime",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Suffix output with datetime at startup of recorder",
    )

    parser.add_argument(
        "--query",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Query router storage for keys before subscribing to them",
    )

    parser.add_argument(
        "--show-frequencies",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="Query router storage for keys before subscribing to them",
    )

    def _parse_pair(arg) -> tuple[pathlib.Path, pathlib.Path]:
        path_to_subject_yaml, path_to_proto_types = arg.split(",")
        return pathlib.Path(path_to_subject_yaml), pathlib.Path(path_to_proto_types) if path_to_proto_types else None

    parser.add_argument(
        "--extra-subjects-types",
        type=_parse_pair,
        action="append",
        help="Add additional well-known subjects and protobuf types as --extra-subjects-types=path/to/subjects.yaml,path_to_protobuf_file_descriptor_set.bin"
    )

    # Parse arguments and start doing our thing
    args = parser.parse_args()

    # Setup logger
    logging.basicConfig(
        format="%(asctime)s %(levelname)s %(name)s %(message)s", level=args.log_level
    )
    logging.captureWarnings(True)

    # Loading extra well-known subjects and types if provided
    if extra_paths := args.extra_subjects_types:
        for pair in extra_paths:
            logger.info("Loading extra subjects (%s) and types (%s)", *pair)
            keelson.add_well_known_subjects_and_proto_definitions(
                *pair
            )

    # Put together zenoh session configuration
    conf = zenoh.Config()

    if args.mode is not None:
        conf.insert_json5("mode", json.dumps(args.mode))
    if args.connect is not None:
        conf.insert_json5("connect/endpoints", json.dumps(args.connect))

    # Construct session
    logger.info("Opening Zenoh session...")
    session = zenoh.open(conf)

    def _on_exit():
        session.close()

    atexit.register(_on_exit)

    run(session, args)


@contextmanager
def ignore(*exception):
    try:
        yield
    except exception as e:
        logger.exception("Something went wrong in the listener!", exc_info=e)


@contextmanager
def mcap_writer(file_handle):
    try:
        writer = Writer(file_handle)
        writer.start()
        logger.info("MCAP writer initilized")
        yield writer
    finally:
        writer.finish()
        logger.info("MCAP writer finished")


def write_message(
    writer: Writer, channel_id: int, log_time: int, publish_time: int, data: bytes
):
    logger.debug(
        "Writing to file: channel_id=%s, log_time=%s, publish_time=%s",
        channel_id,
        log_time,
        publish_time,
    )
    writer.add_message(
        channel_id=channel_id,
        log_time=log_time,
        publish_time=publish_time,
        data=data,
    )


def run(session: zenoh.Session, args: argparse.Namespace):

    output_path = pathlib.Path(
        str(args.output) + (time.strftime("%Y-%m-%d_%H%M") if args.suffix_with_datetime else "")).with_suffix(".mcap")
    logger.info("Using output path: %s", output_path)

    queue = Queue()
    close_down = Event()
    message_counter = Counter()

    def _recorder():
        with output_path.open("wb") as fh, mcap_writer(fh) as writer:
            schemas: Dict[str, int] = {}
            channels: Dict[str, int] = {}

            while not close_down.is_set():
                try:
                    sample: zenoh.Sample = queue.get(timeout=0.01)
                except Empty:
                    continue

                with ignore(Exception):
                    key = str(sample.key_expr)
                    logger.debug("Received sample on key: %s", key)

                    # Increment message count for the topic
                    message_counter[key] += 1

                    # Uncover from keelson envelope
                    try:
                        received_at, enclosed_at, payload = keelson.uncover(
                            sample.payload.to_bytes()
                        )
                    except DecodeError:
                        logger.exception(
                            "Key %s did not contain a valid keelson.Envelope: %s",
                            key,
                            sample.payload.to_bytes(),
                        )
                        continue

                    # If this key is known, write message to file
                    if key in channels:
                        logger.debug("Key %s is already known!", key)
                        write_message(
                            writer, channels[key], received_at, enclosed_at, payload
                        )
                        continue

                    # Else, lets start finding out about schemas etc
                    try:
                        subject = keelson.get_subject_from_pubsub_key(key)
                    except ValueError:
                        logger.exception(
                            "Received key did not match the expected format: %s",
                            key,
                        )
                        continue

                    logger.info("Unseen key %s, adding to file", key)

                    # IF we havent already got a schema for this subject
                    if not subject in schemas:
                        logger.debug("Subject %s not seen before", subject)

                        if keelson.is_subject_well_known(subject):
                            logger.info("Subject %s is well-known!", subject)
                            # Get info about the well-known subject
                            keelson_schema = keelson.get_subject_schema(
                                subject)

                            file_descriptor_set = (
                                keelson.get_protobuf_file_descriptor_set_from_type_name(
                                    keelson_schema
                                )
                            )
                            schemas[subject] = writer.register_schema(
                                name=keelson_schema,
                                encoding=SchemaEncoding.Protobuf,
                                data=file_descriptor_set.SerializeToString(),
                            )

                        else:
                            logger.info(
                                "Unknown subject, storing without schema..."
                            )
                            schemas[subject] = writer.register_schema(
                                name=subject,
                                encoding=SchemaEncoding.SelfDescribing,
                                data=b"",
                            )

                    # Now we have a schema_id, moving on to registering a channel
                    schema_id = schemas[subject]

                    logger.debug(
                        "Registering a channel (%s) with schema_id=%s",
                        key,
                        schema_id,
                    )

                    channels[key] = writer.register_channel(
                        topic=key,
                        message_encoding=MessageEncoding.Protobuf,
                        schema_id=schema_id,
                    )

                    # Finally, put the sample on the queue
                    logger.debug("...and writing the actual message to file!")
                    write_message(
                        writer, channels[key], received_at, enclosed_at, payload
                    )

    recorder_thread = Thread(target=_recorder)
    recorder_thread.daemon = True
    recorder_thread.start()

    if args.query:
        logger.info("Querying the infrastructure for latest values!")

        def _receiver(reply: zenoh.Reply):
            with ignore(Exception):
                queue.put(reply.ok)

        for key in args.key:
            session.get(key, _receiver,
                        consolidation=zenoh.ConsolidationMode.LATEST)

    # And start subscribing
    logger.info("Starting subscribers")
    subscribers = [session.declare_subscriber(
        key, queue.put) for key in args.key]

    while True:
        try:
            # Check queue size
            qsize = queue.qsize()
            logger.debug(f"Approximate queue size is: {qsize}")

            if qsize > 100:
                warnings.warn(f"Queue size is {qsize}")
            elif qsize > 1000:
                raise RuntimeError(
                    f"Recorder is not capable of keeping up with data flow. Current queue size is {qsize}. Exiting!"
                )

            if args.show_frequencies:
                to_print = [
                    f"Key: {key}, Frequency: {count / MAIN_LOOP_SLEEP_TIME:.2f} Hz" for key, count in message_counter.items()
                ]
                if to_print:
                    print(
                        "==== Average frequencies of received data over last 10 s ====")
                    print("\n".join(to_print))

            message_counter.clear()  # Reset counts after logging

            time.sleep(MAIN_LOOP_SLEEP_TIME)
        except KeyboardInterrupt:
            logger.info("Closing down on user request!")
            logger.debug("Undeclaring subscribers...")
            for sub in subscribers:
                sub.undeclare()

            logger.debug("Waiting for all items in queue to be processed...")
            while not queue.empty():
                time.sleep(0.1)

            logger.debug("Joining recorder thread...")
            close_down.set()
            recorder_thread.join()

            logger.debug("Done! Good bye :)")
            break


if __name__ == "__main__":
    main()
